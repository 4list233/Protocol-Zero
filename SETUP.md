# Quick Setup Guide

## âœ… Monorepo Created Successfully!

Your Protocol Zero integrated platform is now set up at:
```
~/Documents/protocol-zero/
```

## ğŸš€ Next Steps

### 1. Set Up GitHub Repository

```bash
cd ~/Documents/protocol-zero

# Create a new repository on GitHub, then:
git remote add origin https://github.com/YOUR_USERNAME/protocol-zero.git
git push -u origin main
```

### 2. Set Up Scraper

```bash
cd scraper

# Create Python virtual environment
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# One-time: Login to Taobao
python scraper.py --login-setup
# Follow prompts to log in via QR code or password

# Test scraping (URLs already in taobao_links.txt)
python scraper.py
```

### 3. Set Up Shop

```bash
cd ../shop

# Install Node.js dependencies
npm install

# Copy environment template
cp .env.example .env.local

# Edit .env.local with your credentials:
# - DATABASE_URL (PostgreSQL)
# - Firebase credentials
# - NextAuth secret

# Set up database
npx prisma generate
npx prisma db push

# Start development server
npm run dev
# Visit http://localhost:3000
```

### 4. Test Integration Workflow

```bash
# From project root: ~/Documents/protocol-zero

# Step 1: Run scraper (if you haven't already)
cd scraper
source .venv/bin/activate
python scraper.py

# Step 2: Sync media to shop
cd ../shared/scripts
npm run sync-media

# Step 3: Generate TypeScript products
npm run generate-products

# Step 4: Check shop
cd ../../shop
npm run dev
# Products should now appear at http://localhost:3000/shop
```

## ğŸ“‚ Directory Overview

```
protocol-zero/
â”œâ”€â”€ scraper/              Python scraper
â”‚   â”œâ”€â”€ scraper.py       Main scraper script
â”‚   â”œâ”€â”€ taobao_links.txt Add URLs here
â”‚   â””â”€â”€ .venv/           Python virtual environment (create this)
â”‚
â”œâ”€â”€ shop/                Next.js e-commerce
â”‚   â”œâ”€â”€ app/             Next.js routes
â”‚   â”œâ”€â”€ lib/             Business logic
â”‚   â”‚   â””â”€â”€ products.generated.ts  â† Auto-generated from scraper
â”‚   â”œâ”€â”€ public/images/   â† Synced from shared/media
â”‚   â””â”€â”€ node_modules/    (npm install creates this)
â”‚
â””â”€â”€ shared/              Integration layer
    â”œâ”€â”€ media/           Scraped images (source of truth)
    â”œâ”€â”€ data/            JSON manifests
    â”‚   â”œâ”€â”€ products_manifest.json      â† Scraper output
    â”‚   â”œâ”€â”€ scrape_queue.json           â† Shop â†’ Scraper requests
    â”‚   â””â”€â”€ catalog_index.json          â† Duplicate detection
    â””â”€â”€ scripts/         Sync automation
        â”œâ”€â”€ sync-media.js               Copy images to shop
        â”œâ”€â”€ generate-products.js        Create TypeScript file
        â””â”€â”€ node_modules/               (npm install creates this)
```

## ğŸ”„ Daily Workflow

### Adding New Products

1. Add Taobao URLs to `scraper/taobao_links.txt`
2. Run `python scraper.py`
3. Run `npm run sync-all` from `shared/scripts/`
4. Rebuild shop if needed: `cd shop && npm run build`

### Automated Sync (Optional)

The GitHub Actions workflow at `.github/workflows/sync-products.yml` will:
- Run daily at 2 AM
- Scrape new products
- Sync media
- Generate TypeScript
- Commit changes

To enable, just push to GitHub - no additional setup needed!

## ğŸ› ï¸ Troubleshooting

### Scraper Issues
- **"No module named 'selenium'"**: Activate venv first: `source .venv/bin/activate`
- **ChromeDriver errors**: Delete `chromedriver.broken`, let Selenium Manager handle it
- **Login required**: Run `python scraper.py --login-setup` again

### Shop Issues
- **Database errors**: Run `npx prisma db push` from `shop/`
- **Missing images**: Run `npm run sync-media` from `shared/scripts/`
- **Products not showing**: Check `shop/lib/products.generated.ts` exists

### Sync Script Issues
- **"fs-extra not found"**: Run `npm install` in `shared/scripts/`
- **Permission denied**: Make scripts executable: `chmod +x shared/scripts/*.js`

## ğŸ“š Documentation

- [Main README](README.md) - Complete documentation
- [Scraper README](scraper/README.md) - Scraper details
- [Shop README](shop/README.md) - Shop setup
- [Integration Plan](scraper/INTEGRATION_PLAN.md) - Architecture

## âœ¨ You're All Set!

Your integrated scraper + shop platform is ready to use. Start by running the scraper to populate products, then check them out in the shop!

**Questions?** Check the main README.md or individual project READMEs.
