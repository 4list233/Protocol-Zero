# Protocol Zero Scraper + Shop Integration Plan

**Status**: âœ… **Phases 1-4 Complete** | â³ **Phase 5 Pending** (Automation)

**Last Updated**: November 18, 2025

## ğŸ‰ What's Working Now

âœ… **Monorepo structure** with scraper/, shop/, shared/  
âœ… **Scraper** extracts products with correct prices (åˆ¸å/ä¼˜æƒ å‰)  
âœ… **Gemini AI translation** removes brand names, keeps military designations  
âœ… **Image capture** with hero/gallery/detail stitching  
âœ… **CSV export** with all variant data  
âœ… **Manifest generation** (export_manifest.py)  
âœ… **Media sync** (sync-media.js)  
âœ… **Product generation** (generate-products.js â†’ products.generated.ts)  
âœ… **Bi-directional sync** infrastructure (scrape_queue.json, catalog_index.json)  

## â³ What's Next

1. **Admin UI** for scrape queue management (shop/app/admin/scraper/page.tsx)
2. **GitHub Actions** for automated nightly sync
3. **Error notifications** (email/Slack alerts)
4. **Pipeline documentation** for team onboarding

---

## Current Architecture Analysis âœ… **MONOREPO COMPLETE**

### **Scraper Project** (`/protocol-zero/scraper/`) âœ…
- **Technology**: Python 3.12, Selenium WebDriver
- **Purpose**: Automated Taobao/Tmall data extraction
- **Outputs**: 
  - CSV: `protocol_zero_variants.csv` (product variants with translations & pricing) âœ…
  - Media: `media/product_{index}_{slug}/` (Main/Details/Catalogue images) âœ…
  - Cookies: `taobao_cookies.json` (persistent Taobao login) âœ…
- **Key Features**: 
  - ~~Rule-based Chineseâ†’English translation~~ â†’ **Gemini AI translation** (airsoft/military context) âœ…
  - Per-variant price scraping with CNYâ†’CAD conversion (0.202 rate + $15 shipping) âœ…
  - åˆ¸å/ä¼˜æƒ å‰ price extraction with parent container traversal âœ…
  - Persistent Chrome profile for login preservation âœ…
  - Selenium Manager auto-recovery âœ…
  - Hero/gallery capture with video skipping âœ…
  - Detail image stitching to Details_Long.jpg âœ…

### **Shop Website** (`/protocol-zero/shop/`) âœ…
- **Technology**: Next.js 14 (React), TypeScript, Prisma ORM, PostgreSQL, Firebase
- **Purpose**: E-commerce storefront
- **Current Product System**: 
  - Static TypeScript file: `lib/products.ts` (baseProducts array) âœ…
  - Generated file: `lib/products.generated.ts` (scraped products, already exists!) âœ…
  - Images: `public/images/` (126 existing images) âœ…
  - Product interface: id, sku, title, price_cad, primaryImage, images[], url, category, description, options[] âœ…
- **Key Routes**: `/shop`, `/shop/[id]`, `/cart`, `/checkout`, `/admin` âœ…

### **Shared Resources** (`/protocol-zero/shared/`) âœ…
- **Data**: 
  - `catalog_index.json` âœ… (duplicate detection)
  - `products_manifest.json` âœ… (shop-compatible JSON)
  - `scrape_queue.json` âœ… (shop â†’ scraper requests)
- **Media**: Product images (source of truth) âœ…
- **Scripts**: 
  - `sync-media.js` âœ… (copy media â†’ shop/public/images/)
  - `generate-products.js` âœ… (manifest â†’ products.generated.ts)
  - `export_manifest.py` âœ… (CSV â†’ manifest JSON)

---

## Integration Strategy

### Phase 1: Project Structure Consolidation âœ… **COMPLETE**

#### **Option A: Monorepo** âœ… **IMPLEMENTED**
~~Merge both projects into a unified repository with clear separation:~~ **Already done:**

```
protocol-zero/
â”œâ”€â”€ scraper/                    # Python scraper (moved from Protocol Z Scraper)
â”‚   â”œâ”€â”€ scraper.py
â”‚   â”œâ”€â”€ taobao_links.txt
â”‚   â”œâ”€â”€ taobao_cookies.json
â”‚   â”œâ”€â”€ chrome_profile_selenium/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ .venv/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ shop/                       # Next.js shop (moved from protocol-zero-shop)
â”‚   â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ prisma/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ shared/                     # Shared resources
â”‚   â”œâ”€â”€ media/                  # Scraped product media (symlinked/copied to shop/public/images/)
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ protocol_zero_variants.csv
â”‚   â”‚   â””â”€â”€ products_manifest.json
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ sync-media.js       # Copy media â†’ shop/public/images/
â”‚       â””â”€â”€ generate-products.js # CSV â†’ products.generated.ts
â”‚
â”œâ”€â”€ .gitignore                  # Combined gitignore
â”œâ”€â”€ README.md                   # Master documentation
â””â”€â”€ docker-compose.yml          # Optional: containerized setup

```

**Pros**: 
- Single repo for version control and deployment
- Shared scripts between scraper and shop
- Clear separation of concerns
- Easy CI/CD pipeline

**Cons**: 
- Mixed tech stacks (Python + Node.js)
- Larger repository size

---

#### **Option B: Separate Repos with Shared Storage**
Keep projects separate but connected via shared directory:

```
/Users/5425855/Documents/
â”œâ”€â”€ protocol-zero-scraper/      # Scraper repo (current)
â”‚   â””â”€â”€ output/
â”‚       â”œâ”€â”€ media/ â†’ /shared/media (symlink)
â”‚       â””â”€â”€ protocol_zero_variants.csv â†’ /shared/data/
â”‚
â”œâ”€â”€ protocol-zero-shop/         # Shop repo (current)
â”‚   â””â”€â”€ public/images/ â† synced from /shared/media
â”‚
â””â”€â”€ protocol-zero-shared/       # New shared storage repo
    â”œâ”€â”€ media/                  # Source of truth for product images
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ protocol_zero_variants.csv
    â”‚   â””â”€â”€ products_manifest.json
    â””â”€â”€ sync-scripts/
        â”œâ”€â”€ scraper-to-shared.py
        â””â”€â”€ shared-to-shop.js
```

**Pros**: 
- Independent deployment of scraper vs shop
- Cleaner git history per project
- Can use different CI/CD per project

**Cons**: 
- More complex synchronization
- Need to manage 3 repos instead of 1

---

### Phase 2: Data Flow & Communication Pipeline

#### **Step 2.1: Scraper Output Enhancement**
Modify `scraper.py` to output shop-compatible JSON:

```python
# New output format: products_manifest.json
{
  "last_updated": "2025-11-17T18:30:00Z",
  "products": [
    {
      "id": "molle-pda-mc-cp",
      "sku": "MOLLE-PDA-001",
      "title": "Tactical Vest Universal MOLLE System Phone Navigation Panel",  # Translated
      "price_cad": 24.99,  # (Price CNY Ã— 0.202) + 15
      "primaryImage": "/images/molle-pda-Main.jpg",
      "images": [
        "/images/molle-pda-Main.jpg",
        "/images/molle-pda-Detail_01.jpg",
        ...
      ],
      "url": "https://item.taobao.com/item.htm?id=713575933395",
      "category": "Pouches",
      "description": "Universal MOLLE system with multiple colour options",
      "options": [
        {
          "name": "Colour",
          "values": ["MC Camouflage", "Black", "Wolf Grey", "Ranger Green"]  # Translated
        }
      ],
      "variants": [
        {
          "option": "MC Camouflage",
          "price_cad": 24.99,
          "image": "/images/molle-pda-mc-variant.jpg"
        },
        ...
      ]
    }
  ]
}
```

#### **Step 2.2: Shop Integration Script**
Create `shop/scripts/import-scraped-products.ts`:

```typescript
// Reads products_manifest.json
// Copies media files to public/images/
// Generates lib/products.generated.ts
// Optional: Updates Prisma database if we add Product model
```

---

### Phase 3: Bi-Directional Communication

#### **3.1: Shop â†’ Scraper (Product Wishlist)**
Shop can request scraping of new products:

**File**: `shared/data/scrape_queue.json`
```json
{
  "queue": [
    {
      "url": "https://item.taobao.com/item.htm?id=123456789",
      "category": "Pouches",
      "priority": "high",
      "requested_at": "2025-11-17T18:30:00Z",
      "requested_by": "admin"
    }
  ]
}
```

**Implementation**:
- Admin panel in shop: "Add Product from Taobao URL" form
- Appends to `scrape_queue.json`
- Scraper reads queue, processes URLs, removes completed items

---

#### **3.2: Scraper â†’ Shop (Catalog Sync)**
Scraper detects existing products to avoid duplication:

**File**: `shared/data/catalog_index.json`
```json
{
  "products": {
    "https://item.taobao.com/item.htm?id=713575933395": {
      "id": "molle-pda-mc-cp",
      "last_scraped": "2025-11-17T18:30:00Z",
      "status": "active",
      "variants": 6
    }
  }
}
```

**Logic**:
- Before scraping, check if URL exists in `catalog_index.json`
- If exists and `last_scraped < 7 days`, skip
- If exists and price changed, update existing product
- If new, add to catalog

---

### Phase 4: Automated Sync Workflow

#### **Cron Job / GitHub Actions**
```yaml
# .github/workflows/sync-products.yml
name: Sync Scraped Products
on:
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM
  workflow_dispatch:      # Manual trigger

jobs:
  scrape-and-sync:
    runs-on: ubuntu-latest
    steps:
      - name: Run Python Scraper
        run: python scraper/scraper.py
      
      - name: Copy Media to Shop
        run: |
          rsync -av shared/media/ shop/public/images/
      
      - name: Generate Products TypeScript
        run: node shared/scripts/generate-products.js
      
      - name: Commit Changes
        run: |
          git add shop/lib/products.generated.ts shop/public/images/
          git commit -m "chore: sync scraped products [skip ci]"
          git push
```

---

## Detailed Implementation Steps

### Step 1: Create Monorepo Structure
```bash
# Create new monorepo
mkdir -p ~/Documents/protocol-zero
cd ~/Documents/protocol-zero

# Move scraper
mv ~/Documents/Protocol\ Z\ Scraper scraper
cd scraper
# Keep: scraper.py, taobao_links.txt, chrome_profile_selenium/, .venv/
# Move media/ â†’ ../shared/media/
# Move protocol_zero_variants.csv â†’ ../shared/data/

# Move shop
cd ~/Documents/protocol-zero
mv ~/Documents/protocol-zero-shop shop

# Create shared directory
mkdir -p shared/{media,data,scripts}

# Initialize git
git init
git remote add origin https://github.com/4list233/protocol-zero.git
```

### Step 2: Update Scraper Output
Add to `scraper.py`:
```python
def export_products_manifest(all_scraped_data):
    """Export shop-compatible JSON manifest"""
    manifest = {
        "last_updated": datetime.now().isoformat(),
        "products": []
    }
    
    # Group variants by product
    products_map = {}
    for row in all_scraped_data:
        url = row['URL']
        if url not in products_map:
            products_map[url] = {
                "id": slugify(row['Translated Title'])[:50],
                "sku": f"AUTO-{len(products_map) + 1:03d}",
                "title": row['Translated Title'],
                "price_cad": row['Final CAD'],
                "primaryImage": f"/images/{row['Media Folder']}/Main.jpg",
                "images": [],
                "url": url,
                "category": categorize_product(row['Translated Title']),
                "variants": []
            }
        
        products_map[url]["variants"].append({
            "option": row['Option Name'],
            "price_cad": row['Final CAD']
        })
    
    manifest["products"] = list(products_map.values())
    
    with open('../shared/data/products_manifest.json', 'w', encoding='utf-8') as f:
        json.dump(manifest, f, ensure_ascii=False, indent=2)
```

### Step 3: Create Media Sync Script
`shared/scripts/sync-media.js`:
```javascript
const fs = require('fs-extra');
const path = require('path');

async function syncMedia() {
  const sourceDir = path.join(__dirname, '../media');
  const targetDir = path.join(__dirname, '../../shop/public/images');
  
  console.log('Syncing media files...');
  await fs.copy(sourceDir, targetDir, { overwrite: true });
  console.log('âœ… Media sync complete');
}

syncMedia().catch(console.error);
```

### Step 4: Create Product Generator Script
`shared/scripts/generate-products.js`:
```javascript
const fs = require('fs');
const path = require('path');

function generateProducts() {
  const manifestPath = path.join(__dirname, '../data/products_manifest.json');
  const outputPath = path.join(__dirname, '../../shop/lib/products.generated.ts');
  
  const manifest = JSON.parse(fs.readFileSync(manifestPath, 'utf-8'));
  
  const tsContent = `import type { Product } from './products'

// Auto-generated from scraper output on ${manifest.last_updated}
// DO NOT EDIT MANUALLY - Changes will be overwritten

export const generatedProducts: Product[] = ${JSON.stringify(manifest.products, null, 2)}
`;

  fs.writeFileSync(outputPath, tsContent, 'utf-8');
  console.log('âœ… Generated products.generated.ts');
}

generateProducts();
```

### Step 5: Add Admin Interface in Shop
`shop/app/admin/scraper/page.tsx`:
```typescript
'use client'

export default function ScraperAdmin() {
  const [url, setUrl] = useState('')
  const [category, setCategory] = useState('Pouches')
  
  async function requestScrape() {
    // Read scrape_queue.json
    // Append new request
    // Write back
    await fetch('/api/scraper/queue', {
      method: 'POST',
      body: JSON.stringify({ url, category })
    })
  }
  
  return (
    <div>
      <h1>Scraper Management</h1>
      <form onSubmit={requestScrape}>
        <input 
          placeholder="Taobao URL" 
          value={url} 
          onChange={e => setUrl(e.target.value)} 
        />
        <select value={category} onChange={e => setCategory(e.target.value)}>
          <option>Pouches</option>
          <option>Grenades</option>
          <option>Radio PTT</option>
        </select>
        <button type="submit">Add to Scrape Queue</button>
      </form>
    </div>
  )
}
```

---

## Migration Checklist

### Phase 1: Setup âœ… **COMPLETE**
- [x] Create monorepo structure
- [x] Move scraper â†’ `scraper/`
- [x] Move shop â†’ `shop/`
- [x] Create `shared/` directory
- [x] Update `.gitignore` to exclude venv, node_modules, .next, media cache
- [x] Test both projects still run independently

### Phase 2: Scraper Enhancement âœ… **COMPLETE**
- [x] Add `products_manifest.json` export to scraper (via export_manifest.py)
- [x] Update CSV to include all shop-required fields
- [x] Add media output to `shared/media/`
- [x] Test scraper with 1 product
- [x] Validate JSON output format
- [x] **BONUS**: Add Gemini AI translation with airsoft/military context
- [x] **BONUS**: Implement åˆ¸å/ä¼˜æƒ å‰ price extraction
- [x] **BONUS**: Detail image stitching (Details_Long.jpg)

### Phase 3: Shop Integration âœ… **COMPLETE**
- [x] Create `sync-media.js` script
- [x] Create `generate-products.js` script
- [x] Test manual sync workflow
- [x] Verify images appear in shop
- [x] Verify products load correctly

### Phase 4: Bi-Directional Sync âœ… **COMPLETE**
- [x] Create `scrape_queue.json` structure
- [ ] Add admin UI for adding URLs â³ **TODO**
- [x] Update scraper to read queue (infrastructure ready)
- [x] Create `catalog_index.json`
- [x] Add duplicate detection logic

### Phase 5: Automation â³ **PENDING**
- [ ] Set up GitHub Actions workflow
- [ ] Test automated sync
- [ ] Add error notifications (email/Slack)
- [ ] Document the full pipeline

---

## File Structure After Integration

```
protocol-zero/
â”œâ”€â”€ .git/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml (optional)
â”‚
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ scraper.py
â”‚   â”œâ”€â”€ taobao_links.txt
â”‚   â”œâ”€â”€ taobao_cookies.json
â”‚   â”œâ”€â”€ chrome_profile_selenium/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ .venv/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ shop/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ admin/scraper/page.tsx (NEW)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ products.ts
â”‚   â”‚   â””â”€â”€ products.generated.ts (auto-generated)
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ images/ (synced from shared/media/)
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ README.md
â”‚
â””â”€â”€ shared/
    â”œâ”€â”€ media/                          # Source of truth
    â”‚   â”œâ”€â”€ product_1_molle-pda/
    â”‚   â”‚   â”œâ”€â”€ Main.jpg
    â”‚   â”‚   â”œâ”€â”€ Detail_01.jpg
    â”‚   â”‚   â””â”€â”€ ...
    â”‚   â””â”€â”€ product_2_m67-grenade/
    â”‚       â””â”€â”€ ...
    â”‚
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ protocol_zero_variants.csv
    â”‚   â”œâ”€â”€ products_manifest.json       # Shop-compatible JSON
    â”‚   â”œâ”€â”€ scrape_queue.json            # Shop â†’ Scraper requests
    â”‚   â””â”€â”€ catalog_index.json           # Duplicate detection
    â”‚
    â””â”€â”€ scripts/
        â”œâ”€â”€ sync-media.js                # Media â†’ shop/public/images/
        â”œâ”€â”€ generate-products.js         # JSON â†’ products.generated.ts
        â””â”€â”€ watch-queue.py               # Monitor scrape_queue.json

```

---

## Benefits of This Architecture

âœ… **Separation of Concerns**: Scraper and shop remain independent  
âœ… **Single Source of Truth**: `shared/` directory is authoritative  
âœ… **Version Control**: All changes tracked in one repo  
âœ… **Automated Sync**: No manual copy-paste needed  
âœ… **Bi-Directional**: Shop can request new products, scraper reports status  
âœ… **Scalable**: Easy to add more data sources (e.g., AliExpress scraper)  
âœ… **Type-Safe**: TypeScript generated from scraper output  
âœ… **Media Management**: Organized, deduplicated image library  

---

## Next Steps

Which option do you prefer?
1. **Monorepo (Option A)** - Single unified repository
2. **Separate Repos (Option B)** - Three repos with shared storage

Once you decide, I'll:
1. Create the directory structure
2. Move files without conflicts
3. Set up the sync scripts
4. Test the integration pipeline
5. Add admin UI for scrape queue management

Let me know and I'll proceed with the implementation!
